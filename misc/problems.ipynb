{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums=[2,7,11,15]\n",
    "target = 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sum(arr: list,k: int):\n",
    "    hashMap = {}\n",
    "    for i in range(len(nums)):\n",
    "        j = k - nums[i]\n",
    "        if j in hashMap.keys():\n",
    "            return i,hashMap[j]\n",
    "        else:\n",
    "            hashMap[nums[i]] = i\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sum(nums,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h   1\n",
      "e   1\n",
      "l   3\n",
      "o   2\n",
      "    1\n",
      "w   1\n",
      "r   1\n",
      "d   1\n"
     ]
    }
   ],
   "source": [
    "str1 = 'Hello World'\n",
    "hmap = {}\n",
    "\n",
    "str1 = str1.lower()\n",
    "\n",
    "for char in str1:\n",
    "    if char in hmap:\n",
    "        hmap[char] += 1\n",
    "    else:\n",
    "        hmap[char] = 1\n",
    "\n",
    "for k,v in hmap.items():\n",
    "    print(k,\" \"|v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 7, 9, 10] [2, 3, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "arr1 = [7|5|1|10|9]\n",
    "arr2 = [3|8|2|6|4]\n",
    "\n",
    "for i in range(len(arr1)):\n",
    "    for j in range(0|len(arr1)-1):\n",
    "        if arr1[j] > arr1[j+1]:\n",
    "            arr1[j]|arr1[j+1] = arr1[j+1]|arr1[j]\n",
    "\n",
    "for i in range(len(arr2)):\n",
    "    for j in range(0|len(arr2)-1):\n",
    "        if arr2[j] > arr2[j+1]:\n",
    "            arr2[j]|arr2[j+1] = arr2[j+1]|arr2[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "lst = [0] * (len(arr1)+len(arr2))\n",
    "left = 0\n",
    "right = 0 \n",
    "index = 0\n",
    "\n",
    "while (left < len(arr1)) and (right < len(arr2)):\n",
    "    if arr1[left] < arr2[right]:\n",
    "        lst[index] = arr1[left]\n",
    "        left += 1\n",
    "        index += 1\n",
    "    else:\n",
    "        lst[index] = arr2[right]\n",
    "        right += 1\n",
    "        index += 1\n",
    "while left < len(arr1):\n",
    "    lst[index:] = arr1[left:]\n",
    "    break\n",
    "\n",
    "while right < len(arr2):\n",
    "    lst[index] = arr2[right:]\n",
    "    break\n",
    "\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/27 19:53:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = ['id','name','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "            .option('header','false')\\\n",
    "            .option('schema',my_schema)\\\n",
    "            .option('mode','FAILFAST')\\\n",
    "            .load('/home/mohitkatkar/Documents/mohit.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|_c0|   _c1|       _c2|\n",
      "+---+------+----------+\n",
      "|  1| Mohit|01-01-2022|\n",
      "|  2|Katkar|11-11-2022|\n",
      "|  3| Adesh|12-03-2022|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkParenthesis(exper:str):\n",
    "    stack = []\n",
    "    for char in exper:\n",
    "        if char in ['(','{','[']:\n",
    "            stack.append(char)\n",
    "        else:\n",
    "            if not stack:\n",
    "                return False\n",
    "            current_char = stack.pop()\n",
    "            if current_char == '(':\n",
    "                if char != ')':\n",
    "                    return False\n",
    "            if current_char == '[':\n",
    "                if char != ']':\n",
    "                    return False\n",
    "            if current_char == '{':\n",
    "                if char != '}':\n",
    "                    return False\n",
    "    if stack:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkParenthesis('[{}]{({})}()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkParenthesis(expr:str):\n",
    "    stack = []\n",
    "    hashM = {'}':'{',')':'(',']':'['}\n",
    "\n",
    "    for char in expr:\n",
    "        if char in ['(','{','[']:\n",
    "            stack.append(char)\n",
    "        \n",
    "        else:\n",
    "            if not stack:\n",
    "                return False\n",
    "            current_char = stack.pop()\n",
    "\n",
    "            if hashM[char] != current_char:\n",
    "                return False\n",
    "            \n",
    "    if stack:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkParenthesis('[{}]{({})}()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instert_num(arr,n):\n",
    "    index = len(arr)\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] > n:\n",
    "            index = i\n",
    "            break\n",
    "    \n",
    "    if index == len(arr):\n",
    "        arr = arr[:index] + [n]\n",
    "    else:\n",
    "        arr = arr[:index] + [n] + arr[index:]\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 8, 12, 15, 17]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instert_num([3,6,8,12,15],17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_palindrome(strn:str):\n",
    "    left = 0 \n",
    "    right = len(strn) - 1\n",
    "\n",
    "    while left < right:\n",
    "        if strn[left] == strn[right]:\n",
    "            left += 1\n",
    "            right -= 1\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_palindrome('ABB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"123a!\"\n",
    "\n",
    "def create_duplicate(s:str):\n",
    "    new_char = \"\"\n",
    "    for char in s:\n",
    "        new_char = new_char + (char * 2)\n",
    "    return new_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'112233aa!!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_duplicate(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n:int):\n",
    "    for i in range(2, n//2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_prime(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/29 15:45:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 15:45:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('sql_test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= [(1,'Arul','SQL'),\n",
    "(1,'Arul','Spark'),\n",
    "(2,'Bhumica','SQL'),\n",
    "(2,'Bhumica','Spark')]\n",
    "\n",
    "schema= ['id','name','course']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|course|\n",
      "+---+-------+------+\n",
      "|  1|   Arul|   SQL|\n",
      "|  1|   Arul| Spark|\n",
      "|  2|Bhumica|   SQL|\n",
      "|  2|Bhumica| Spark|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"\"\"\n",
    "select id, name, array_join(collect_list(course),',')  as courses\n",
    "from table group by id, name\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|   name|  courses|\n",
      "+---+-------+---------+\n",
      "|  1|   Arul|SQL,Spark|\n",
      "|  2|Bhumica|SQL,Spark|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('table2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|course|\n",
      "+---+-------+------+\n",
      "|  1|   Arul|   SQL|\n",
      "|  1|   Arul| Spark|\n",
      "|  2|Bhumica|   SQL|\n",
      "|  2|Bhumica| Spark|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select id, name, explode(split(courses,',')) as course from table2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema=['id','name','sal','mngr_id']\n",
    "\n",
    "manager_df= spark.createDataFrame(data=data,schema=schema)\n",
    "manager_df.createOrReplaceTempView(\"manager_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manager_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------+\n",
      "|manager_name|manager_id|avg_sal|\n",
      "+------------+----------+-------+\n",
      "|        Anil|        10|90000.0|\n",
      "|      Rajesh|        16|65000.0|\n",
      "|       Raman|        17|62500.0|\n",
      "|         Sam|        18|53750.0|\n",
      "+------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as(\n",
    "select distinct mngr_id, avg(sal) over(partition by mngr_id) as avg_sal from manager_tbl\n",
    ")\n",
    "select m.name as manager_name, c.mngr_id as manager_id, c.avg_sal\n",
    "from cte c inner join manager_tbl m\n",
    "on c.mngr_id = m.id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------+\n",
      "|manager_name|manager_id|avg_sal|\n",
      "+------------+----------+-------+\n",
      "|        Anil|        10|90000.0|\n",
      "|      Rajesh|        16|65000.0|\n",
      "|       Raman|        17|62500.0|\n",
      "|         Sam|        18|53750.0|\n",
      "+------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as(\n",
    "select mngr_id, avg(sal) as avg_sal from manager_tbl\n",
    "group by mngr_id\n",
    ")\n",
    "select m.name as manager_name, c.mngr_id as manager_id, c.avg_sal\n",
    "from cte c inner join manager_tbl m\n",
    "on c.mngr_id = m.id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1,2,3,4,5]\n",
    "list2 = list1\n",
    "list3 =list1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1.append(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3.append(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2.append(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'a':17, 'b':18, 'c':16, 'd':2}\n",
    "\n",
    "sorted_dict = {}\n",
    "\n",
    "list_of_sorted_keys = sorted(my_dict, key=my_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'c', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_sorted_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_sorted_keys:\n",
    "    sorted_dict[i] = my_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 2, 'c': 16, 'a': 17, 'b': 18}\n"
     ]
    }
   ],
   "source": [
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "file_path = '/home/mohitkatkar/Documents/file.csv'\n",
    "\n",
    "my_schema =  StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Department\",StringType(),True),\n",
    "    StructField(\"Salary\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "        .option('header','true')\\\n",
    "        .option('sep','|')\\\n",
    "        .schema(my_schema)\\\n",
    "        .load(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|    Name|Department|Salary|\n",
      "+--------+----------+------+\n",
      "|    John|     Sales| 35000|\n",
      "|    Jane| Marketing| 15000|\n",
      "|   Alice|        IT| 19000|\n",
      "|Williams|        HR| 50000|\n",
      "|   Brown|   Finance| 70000|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(col(\"Salary\")<20000)\n",
    "\n",
    "df3 = df2.withColumn(\"Bonus\",col('Salary')*.1)\n",
    "\n",
    "df4 = df3.withColumn(\"Total_Salary\",col(\"Salary\")+col(\"Bonus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.withColumnRenamed('Salary',\"Employee_Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+------+------------+\n",
      "| Name|Department|Salary| Bonus|Total_Salary|\n",
      "+-----+----------+------+------+------------+\n",
      "| Jane| Marketing| 15000|1500.0|     16500.0|\n",
      "|Alice|        IT| 19000|1900.0|     20900.0|\n",
      "+-----+----------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.write.format('parquet').save(\"file_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('lastName',when(col('lastName').contains(\"!\"),\"\").otherwise('lastName'))\\\n",
    "    .withColumn('lastName',when(col('lastName').contains(\"$\"),\"\").otherwise('lastName'))\\\n",
    "    .withColumn('lastName',when(col('lastName').contains(\"@\"),\"\").otherwise('lastName'))\\\n",
    "    .withColumn('lastName',when(col('lastName').contains(\"#\"),\"\").otherwise('lastName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('lastName',regexp_replace(col('lastName'),\"[!@#$]\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[12,3,27,5,4,9,4]\n",
    "k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_mul(lst,k):\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(len(lst)-1):\n",
    "            x = lst[i]*lst[j]\n",
    "            if (k/x) in lst[i+2:]:\n",
    "                return lst[i],lst[j],(k//x)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_mul(lst,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mul_hm(lst,k):\n",
    "    hashMap = {}\n",
    "\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(i,len(lst)-1):\n",
    "            x = lst[i] * lst[j]\n",
    "            if k/x in hashMap:\n",
    "                return i,j,hashMap[k//x]\n",
    "            else:\n",
    "                hashMap[lst[i]] = i\n",
    "                hashMap[lst[j]] = j\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_mul_hm(lst,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anagram(str1,str2):\n",
    "    hashMap1 = {}\n",
    "    hashMap2 = {}\n",
    "\n",
    "    if len(str1) !=  len(str2):\n",
    "        return False\n",
    "\n",
    "    for i in range(len(str1)):\n",
    "        hashMap1[str1[i]] = 1 + hashMap1.get(str1[i],0)\n",
    "\n",
    "    for i in range(len(str2)):\n",
    "        hashMap2[str2[i]] = 1 + hashMap2.get(str2[i],0)\n",
    "    \n",
    "    if hashMap1 == hashMap2:\n",
    "        return True\n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = 'abd'\n",
    "str2 = 'dcb'\n",
    "check_anagram(str1,str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
